---
title: "Parameter Recovery GCM"
author: "Jana Jarecki & Kilian Sennrich"
date: "11 8 2020"
output: 
  html_document:
    theme: united
    toc: true
    toc_float: true
---
```{r, echo=TRUE, results="hide", message=FALSE}
#Libraries
pkgs <- c("data.table",
          "tidyverse",
          "Hmisc",
          "psych",
          "qgraph")

lapply(pkgs, library, character.only = TRUE)
```

```{r, echo=TRUE, results="hide", message=FALSE}
#Data // l = long; w = wide
data.l <- readRDS("../../data/raw/recovery_results.RDS")
data.l <- data.l[convergence != 1][, convergence := NULL]
data.w <- dcast(
  data = data.l,
  formula = run + discount + nblock + type + row ~ names,
  value.var = c("par", "true_par"))
data.w <- data.w[true_par_lambda == 1]
```

There are 31 cases, where the GCM could not find optimal free parameters.

## 1. What interdependencies can be found between the variables?

In the following, I compile a correlation matrix to see, whether there are interdependencies between different variables. Significant correlation indicates some kind of systematic dependency between the two variables involved. First, "row" and "run" columns are deleted from the data frame, that is because "run" is not expected to have significant influence and because all information of the "row" column if fully covered by the "true_par_lambda" and "true_par_tau" columns. Second, Spearman correlation is used to compile the correlation matrix. The nonparametric method is used, since not all the data is expected to meet criteria for pearson correlation. Third, correlation matrix is transformed to be in a more accessible format. Fourth, significant p-values are filtered. Knowing interdependencies gathers evidence for the decision to plot and test data either marginalized or conditioned. No correction method is used to control for alpha inflation.

```{r, echo=TRUE, results="hide", message=FALSE}
#Prepare data for correlation matrix
data.w.cor <- data.w[, -c("row", "run")]

#correlation matrix to check for interdependencies between the variables. Nonparametric method is used, since data did not meet criteria for pearson
cormat <- rcorr(as.matrix(data.w.cor), type = "spearman")
cormat.r <- as.data.frame(t(cormat$r))
cormat.p <- as.data.frame(cormat$P)
data.cor <-data.table(
  Var1 = rownames(cormat.r)[row(cormat.r)[upper.tri(cormat.r)]], 
  Var2 = colnames(cormat.r)[col(cormat.r)[upper.tri(cormat.r)]], 
  corr=round(cormat.r[upper.tri(cormat.r)],4),
  p = round(cormat.p[upper.tri(cormat.p)],4))

#filter relevant values
data.cor.rel <- data.cor[corr != 0 & corr != 1 & corr != -1 & p < 0.05]
```

### Graphical analysis of the intercorrelations between relevant variables

Correlation plots are drawn to check for partial correlations. Spearman rank correlation is used as the correlation method. 

#### correlation plots with qgraph

```{r, echo=TRUE}
#subset data
data.qgraph.post <- data.w[,c("par_b0", "par_b1", "par_color", "par_lambda", "par_shape", "par_size", "par_tau")]
data.qgraph.pre <- data.w[, c("discount", "type", "nblock", "true_par_tau")]

#transform into desired format
data.qgraph.post <- cor(data.qgraph.post, use = "pairwise.complete.obs", method = "spearman")
data.qgraph.pre <- cor(data.qgraph.pre, use = "pairwise.complete.obs", method = "spearman")

#graphs and correlation matrices
data.qgraph.post
qgraph(data.qgraph.post, graph = "pcor", layout = "spring")
data.qgraph.pre
qgraph(data.qgraph.pre, graph = "pcor", layout = "spring")

```

### Testing wether discount, nblock and type have significant influence on the recovery of tau.

```{r, echo=TRUE}
#dataframe with all possible combinations
combinations <- expand.grid(
  disc1 = c(0, 8),
  disc2 = c(0, 8),
  nblo1 = c(30, 100),
  nblo2 = c(30, 100),
  type1 = 1:6,
  type2 = 1:6)

#dataframe with all combinations of interest
combinations <- subset(
  combinations, 
  disc1 != disc2 &
  nblo1 == nblo2 &
  type1 == type2 | 
  disc1 == disc2 &
  nblo1 != nblo2 &
  type1 == type2 |
  disc1 == disc2 &
  nblo1 == nblo2 &
  type1 != type2,
  drop = TRUE)

results <- list()
l = 1

for (i in 1:nrow(combinations)) {
  #filter every row separatly and subset values
  row <- combinations[i,]
  disc1 <- as.double(row["disc1"])
  disc2 <- as.double(row["disc2"])
  nblo1 <- as.double(row["nblo1"])
  nblo2 <- as.double(row["nblo2"])
  type1 <- as.double(row["type1"])
  type2 <- as.double(row["type2"])
  
  #make components for rtest
  component_1 <- data.w[
    discount == disc1 &
    nblock == nblo1 &
    type == type1]
  component_2 <- data.w[
    discount == disc2 &
    nblock == nblo2 &
    type == type2]
  
  #spearman and pearson correlations
  cor.c1.spearman <- cor(
    component_1$par_tau,
    component_1$true_par_tau,
    method = "spearman")
  cor.c2.spearman <- cor(
    component_2$par_tau,
    component_2$true_par_tau,
    method = "spearman")
  cor.c1.pearson <- cor(
    component_1$par_tau,
    component_1$true_par_tau,
    method = "pearson")
  cor.c2.pearson <- cor(
    component_2$par_tau,
    component_2$true_par_tau,
    method = "pearson")
  
  #correlation tests for spearman and pearson
  rtest.spearman <- r.test(
    n = nrow(component_1),
    n2 = nrow(component_2),
    r12 = cor.c1.spearman,
    r34 = cor.c2.spearman)
  rtest.pearson <- r.test(
    n = nrow(component_1),
    n2 = nrow(component_2),
    r12 = cor.c1.pearson,
    r34 = cor.c2.pearson)
  
  #list with all information of interest
  results[[l]] <- data.table(
    discount_1 = disc1,
    discount_2 = disc2,
    nblock_1 = nblo1,
    nblock_2 = nblo2,
    type_1 = type1,
    type_2 = type2,
    spearman_1 = cor.c1.spearman,
    spearman_2 = cor.c2.spearman,
    pearson_1 = cor.c1.pearson,
    pearson_2 = cor.c2.pearson,
    mse_1 = mean((component_1$true_par_tau - component_1$par_tau)^2),
    mse_2 = mean((component_2$true_par_tau - component_2$par_tau)^2),
    p.spearman = round(rtest.spearman$p, 4),
    z.spearman = round(rtest.spearman$z, 4),
    p.pearson = round(rtest.pearson$p, 4),
    z.pearson = round(rtest.pearson$z, 4)
  )
  l = l + 1
}

#convert results into data.table
results <- rbindlist(results)
results <- results[p.spearman < 0.05 | p.pearson < 0.05][order(p.pearson)][-seq(2, 68, by = 2)]

# adjust p-values
results$adj_spearman <- p.adjust(
  results$p.spearman,
  method = "bonferroni")
results$adj_pearson <- p.adjust(
  results$p.pearson,
  method = "bonferroni")

#possibility for alpha inflation error
1 - (1 - 0.05)^nrow(results)

#pvalue after bonferroni
0.05/34

results[adj_spearman < 0.05]
results[adj_pearson < 0.05]
```

**We see only differences between type 1, 2 and 6 and 3,4 and 5 are signifficant, suggesting that only different types of shepards categories moderate the recovery of tau.**

Limitations: To even get significant results, i first filtered the pvalues that are significant without bonferroni. Only in a second step i corrected the pvalues with bonferroni. I think the results are still accurate, because it pretty much goes with the visual results we obtain further down the page. I think to report it would be good to present a correlation table with corrected and uncorrected p values. Dividing every pvalue trough 34 results in a p value of 0.00147 that has to be passed by every group. Bonferroni is in my eyes a tradeoff between understanding the data propperly and statistical accuracy. I think in our case its ok to take a possibility of p = 0.83 to at least have one result that wrongfully reports statistical significance where there is none.

### Understanding the recovery of type 1
```{r, echo=TRUE, results="hide", message=FALSE}
#Rendered with dimension "color" == 0.98
data.type1 <- readRDS("../../data/raw/results_type1.RDS")
data.type1 <- data.type1[convergence != 1][, convergence := NULL]
data.type1 <- dcast(
  data = data.type1,
  formula = run + discount + nblock + type + row ~ names,
  value.var = c("par", "true_par"))

data.type1 <- data.type1 %>% 
  group_by(discount, nblock, true_par_tau) %>% 
  mutate(med_type1_new = median(par_color))

#Rendered with dimension "color" == 0.33
dat.type1.orig <- data.w[type == 1]

dat.type1.orig <- dat.type1.orig %>% 
  group_by(discount, nblock, true_par_tau) %>% 
  mutate(med_type1 = median(par_color))
```

#### true_par_color == 0.98

```{r, echo=TRUE}
ggplot(
  data = data.type1,
  mapping = aes(y = par_color))+
  geom_histogram(bins = 60) +
  geom_hline(aes(yintercept = true_par_color), color = "red") +
  geom_hline(aes(yintercept = med_type1_new), color="blue", linetype="dashed")+
  facet_grid(discount + nblock ~ true_par_tau) +
  theme_minimal()
```

#### true_par_color == 0.33

```{r, echo=TRUE}
ggplot(
  data = dat.type1.orig,
  mapping = aes(y = par_color))+
  geom_histogram(bins = 60) +
  geom_hline(aes(yintercept = true_par_color), color = "red") +
  geom_hline(aes(yintercept = med_type1), color="blue", linetype="dashed")+
  facet_grid(discount + nblock ~ true_par_tau) +
  theme_minimal()
```

### Graphical analysis of the recovery of all parameters

For every parameter, dependencies on other variables will be checked. Depending on this, the plots are designed as meaningful as possible. 

#### b0

```{r, echo=TRUE}
#recovery of b0
data.cor.rel[Var1 == "par_b0" | Var2 == "par_b0"]

data.w.b0 <- data.w %>% 
  group_by(type, true_par_tau) %>% 
  mutate(med_b0 = median(par_b0))

ggplot(data = data.w.b0,
       mapping = aes(y = par_b0)) +
  geom_histogram(bins = 100) +
  geom_hline(aes(yintercept = true_par_b0), color = "red") +
  geom_hline(aes(yintercept = med_b0), color="blue", linetype="dashed")+
  facet_grid(true_par_tau ~ type)+
  theme_minimal()
```

#### b1

```{r, echo=TRUE}
#recovery of b1
data.cor.rel[Var1 == "par_b1" | Var2 == "par_b1"]

data.w.b1 <- data.w %>% 
  group_by(type, true_par_tau) %>% 
  mutate(med_b1 = median(par_b1))

ggplot(data = data.w.b1,
       mapping = aes(y = par_b1)) +
  geom_histogram(bins = 100) +
  geom_hline(aes(yintercept = true_par_b1), color = "red") +
  geom_hline(aes(yintercept = med_b1), color="blue", linetype="dashed")+
  facet_grid(true_par_tau ~ type) +
  theme_minimal()
```

#### size

```{r, echo=TRUE}
#recovery of size
data.cor.rel[Var1 == "par_size" | Var2 == "par_size"]

data.w.size <- data.w %>% 
  group_by(type) %>% 
  mutate(med_size = median(par_size))

ggplot(data = data.w.size,
       mapping = aes(y = par_size)) +
  geom_histogram(bins = 100) +
  geom_hline(aes(yintercept = true_par_size), color = "red") +
  geom_hline(aes(yintercept = med_size, group = type), color="blue", linetype="dashed")+
  facet_wrap(~type)+
  theme_minimal()
```

#### color

```{r, echo=TRUE}
#recovery of color
data.cor.rel[Var1 == "par_color" | Var2 == "par_color"]

data.w.color <- data.w %>% 
  group_by(nblock, true_par_lambda, true_par_tau) %>% 
  mutate(med_color = median(par_color))

ggplot(data = data.w.color,
       mapping = aes(y = par_color)) +
  geom_histogram(bins = 100) +
  geom_hline(aes(yintercept = true_par_color), color = "red") +
  geom_hline(aes(yintercept = med_color), color="blue", linetype="dashed")+
  facet_grid( nblock ~ true_par_tau) +
  theme_minimal()
```

#### shape

```{r, echo=TRUE}

#recovery of shape
data.cor.rel[Var1 == "par_shape" | Var2 == "par_shape"]

data.w.shape <- data.w %>%
  group_by(true_par_tau, true_par_lambda, type) %>%
  mutate(med_shape = median(par_shape))

ggplot(data = data.w.shape,
       mapping = aes(y = par_shape)) +
  geom_histogram(bins = 100) +
  geom_hline(aes(yintercept = true_par_shape), color = "red") +
  geom_hline(aes(yintercept = med_shape), color="blue", linetype="dashed") +
  facet_grid(type ~ true_par_tau) +
  theme_minimal()
```

#### lambda

```{r, echo=TRUE}
#recovery of lambda
data.cor.rel[Var1 == "par_lambda" | Var2 == "par_lambda"]

data.w.lambda <- data.w %>%
  group_by(true_par_lambda, true_par_tau, type) %>%
  mutate(med_lambda = median(par_lambda))
         
ggplot(data = data.w.lambda,
       mapping = aes(y = par_lambda)) +
  geom_histogram(bins = 100) +
  geom_hline(aes(yintercept = true_par_lambda), color = "red") +
  geom_hline(aes(yintercept = med_lambda), color="blue", linetype="dashed")+
  facet_grid(true_par_tau ~ type)+
  theme_minimal()
```

#### tau

```{r, echo=TRUE}
#recovery of tau
data.cor.rel[Var1 == "par_tau" | Var2 == "par_tau"]

data.w.tau <- data.w %>% 
  group_by(true_par_tau, nblock) %>% 
  mutate(med_tau = median(par_tau))
  
ggplot(data = data.w.tau,
       mapping = aes(y = par_tau)) +
  geom_histogram(bins = 100) +
  geom_hline(aes(yintercept = true_par_tau), color = "red") +
  geom_hline(aes(yintercept = med_tau), color="blue", linetype="dashed")+ 
  facet_grid(nblock ~ true_par_tau) +
  theme_minimal()
```

## 2. Impact of omitting the first 0 and 8 rows on recovering tau

According to the GCM, omitting the first 3 or 8 rows should lead to better recovery of the tau parameter. Reason being, that the machine learning algorithm does not make accurate decisions on early rows of training.  To further test the effect of omitting the aforementioned rows, spearman correlation coefficients will be compared in a correlation test. Since nblock seems to have impact on par_tau (although correlation is small and could have emerged due to randomness), separate testing for nblock = 30 and nblock = 100 is advised.

#### violin plot for "discount"

```{r, echo=TRUE}
#violin plots for discount per par_tau
ggplot(data = data.w,
       mapping = aes(x = as.factor(discount),
                     y = par_tau)) +
  geom_violin(width = 0.9) +
  stat_summary(fun = "median", 
               geom = "point") +
  geom_line(aes(x = discount, 
                y = true_par_tau), 
            color = "red", 
            size = 0.5) +
  facet_grid(nblock ~ true_par_tau)+
  ylim(0, 10) +
  xlab("Discount") +
  ylab("Tau") +
  theme_minimal()
```

#### line plot for "discount"

```{r, echo=TRUE}
# line plot
ggplot(data = data.w,
       mapping = aes(x = true_par_tau,
                     y = par_tau)) +
  geom_point(aes(x = true_par_tau,
                 y = par_tau),
             alpha = 1 / 20) +
  geom_line(aes(x = true_par_tau,
                y = true_par_tau),
            color = "red") +
  stat_summary(aes(group = discount),
               fun = median,
               geom = "line",
               color = c(rep("green", 6), #discount = 0
                         rep("blue", 6),  #discount = 8
                         rep("green", 6), #discount = 0
                         rep("blue", 6)   #discount = 8
                         ))+
  facet_wrap(~nblock) +
  ylim(0,10) +
  xlab("True tau") +
  ylab("Median of estimated tau") +
  theme_minimal()
```


## 3. Impact of training with multiple blocks on recovery of tau

According to the GCM, the more repetitive blocks a subject will be able to learn with, the better the categorization accuracy should get. Therefore, with increasing blocks, a better recovery is expected. There are no significant dependencies on nblock. Therefore, its ok to not further subset the data. 

#### violinplot for "nblock"

```{r, echo=TRUE}
#violin plots for nblock per par_tau 
ggplot(data = data.w,
       mapping = aes(x = as.factor(nblock),
                     y = par_tau)) +
  geom_violin(width = 0.9) +
  stat_summary(fun = "median", 
               geom = "point") +
  geom_line(aes(x = nblock, 
                y = true_par_tau), 
            color = "red", 
            size = 0.5) +
  facet_grid(nblock ~ true_par_tau)+
  ylim(0, 10) +
  xlab("nblock") +
  ylab("Tau") +
  theme_minimal()
```

#### line plot for "nblock"

```{r, echo=TRUE}
# line plot
ggplot(data = data.w,
       mapping = aes(x = true_par_tau,
                     y = par_tau)) +
  geom_point(aes(x = true_par_tau,
                 y = par_tau),
             alpha = 1 / 20) +
  geom_line(aes(x = true_par_tau,
                y = true_par_tau),
            color = "red") +
  stat_summary(aes(group = nblock),
               fun = median,
               geom = "line",
               color = c(rep("green", 6),  #nblock = 30
                         rep("blue", 6)    #nblock = 100
                         ))+
  ylim(0,10) +
  xlab("True tau") +
  ylab("Median of estimated tau") +
  theme_minimal()
```

## 4. ability of the GCM to grasp the difficulty of the categories on the tau parameter

Shepard suggests, that of 70 possible combinations of elements characterized by 3 dimensions in psychological space, 6 basic types can be derived. The types vary in the dimensions, a subject has to focus on, and therefore can be put into an order of difficulty.Shepard suggests the following order of difficulty:  I < II < ( I I I , IV, V) < VI. If the GCM function works, depending on the type, goodness of recovery of tau paramter should imitate the suggested order. No significant dependency on type can be found.

#### violinplot for "type"

```{r, echo=TRUE}
#violin plots for par_tau per type
ggplot(data = data.w,
       mapping = aes(x = as.factor(type),
                     y = par_tau)) +
  geom_violin(width = 0.9) +
  stat_summary(fun = "median", 
               geom = "point") +
  geom_line(aes(x = par_tau, 
                y = true_par_tau), 
            color = "red", 
            size = 0.5) +
  facet_grid(nblock ~ true_par_tau)+
  ylim(0, 10) +
  xlab("type") +
  ylab("Tau") +
  theme_minimal()
```

#### lineplot for "type"

```{r, echo=TRUE}
# line plot
ggplot(data = data.w,
       mapping = aes(x = true_par_tau,
                     y = par_tau)) +
  geom_point(aes(x = true_par_tau,
                 y = par_tau),
             alpha = 1 / 20) +
  geom_line(aes(x = true_par_tau,
                y = true_par_tau),
            color = "red") +
  stat_summary(aes(group = type),
               fun = median,
               geom = "line",
               color = c(rep("blue", 144)
                         ))+
  facet_grid(nblock + discount ~ type)+
  ylim(0,10) +
  xlab("True tau") +
  ylab("Median of estimated tau") +
  theme_minimal()
```

## 5. Influence of Lambda on the recovery of Tau

Lambda, the speed of learning (sensitivity) is considered to have a positive impact on the recovery of Tau. When a person learns quicker, the recovery of the prabability of implementing an action should be better especially with bigger Tau values.No significant dependencies on true_par_lambda can be found. 

```{r, echo=FALSE, results="hide", message=FALSE}
#Data // l = long; w = wide
data.l <- readRDS("../../data/raw/recovery_results.RDS")
data.l <- data.l[convergence != 1][, convergence := NULL]
data.w <- dcast(
  data = data.l,
  formula = run + discount + nblock + type + row ~ names,
  value.var = c("par", "true_par"))
```

**Graphical analysis**

#### violin plot for "lambda"

```{r, echo=TRUE}
#violin plots for true lambda per par_tau
ggplot(data = data.w,
       mapping = aes(x = as.factor(true_par_lambda),
                     y = par_tau)) +
  geom_violin(width = 0.9) +
  stat_summary(fun = "median", 
               geom = "point") +
  geom_line(aes(x = par_tau, 
                y = true_par_tau), 
            color = "red", 
            size = 0.5) +
  facet_grid(nblock ~ true_par_tau)+
  ylim(0, 10) +
  xlab("Lambda") +
  ylab("Tau") +
  theme_minimal()
```

#### lineplot for "lambda"

```{r, echo=TRUE}
# line plot
ggplot(data = data.w,
       mapping = aes(x = true_par_tau,
                     y = par_tau)) +
  geom_point(aes(x = true_par_tau,
                 y = par_tau),
             alpha = 1 / 20) +
  geom_line(aes(x = true_par_tau,
                y = true_par_tau),
            color = "red") +
  stat_summary(aes(group = true_par_lambda),
               fun = median,
               geom = "line",
               color = c(rep("green", 6),  #lambda = 1
                         rep("blue", 6),   #lambda = 5
                         rep("green", 6),  #lambda = 1
                         rep("blue", 6)    #lambda = 5
                         ))+
  facet_wrap(~nblock)+
  ylim(0,10) +
  xlab("True tau") +
  ylab("Median of estimated tau") +
  theme_minimal()
```