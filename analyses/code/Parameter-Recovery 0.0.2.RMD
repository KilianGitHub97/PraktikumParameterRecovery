---
title: "Parameter Recovery GCM"
author: "Jana Jarecki & Kilian Sennrich"
date: "22 7 2020"
output: 
  html_document:
    theme: united
    toc: yes
---
```{r, echo=TRUE, results="hide", message=FALSE}
#Libraries
pkgs <- c("data.table",
          "tidyverse",
          "emmeans",
          "Hmisc",
          "MASS")

lapply(pkgs, library, character.only = TRUE)
```

```{r, echo=TRUE, results="hide", message=FALSE}
#Data // l = long; w = wide
data.l <- readRDS("../../data/raw/recovery_results.RDS")
data.l <- data.l[convergence != 1][, convergence := NULL]
data.w <- dcast(data = data.l,
                formula = run + discount + nblock + type + row ~ names,
                value.var = c("par", "true_par"))
```

There are '(100800 - nrow(data.l))/7' cases, where the GCM could not find optimal free parameters.

## 1. What interdependencies can be found between the variables?

In the following, I compile a correlation matrix to see, whether there are interdependencies between different variables. Significant correlation indicates some kind of systematic influence between the two variables involved. First, "row" and "run" columns are deleted from the data frame, that is because "run" is not expected to have significant influence and because all information of the "row" column if fully covered by the "true_par_lambda" and "true_par_tau" columns. Second, Spearman correlation is used to compile the correlation matrix. The nonparametric method is used, since not all the data is expected to meet criteria for pearson correlation. Third, correlation matrix is transformed to be in a more accessible format. Fourth, significant p-values are filtered. Knowing interdependencies gathers evidence for the decision to plot and test data either marginalized or conditioned.

**H1: There are significant dependencies between variable x and y**
```{r, echo=TRUE, results="hide", message=FALSE}
#Prepare data for correlation matrix
data.w.cor <- data.w[, -c("row", "run")]

#correlation matrix to check for interdependencies between the variables. Nonparametric method is used, since data did not meet criteria for pearson
cormat <- rcorr(as.matrix(data.w.cor), type = "spearman")
cormat.r <- as.data.frame(t(x$r))
cormat.p <- as.data.frame(x$P)
data.cor <-data.table(Var1 = rownames(cormat.r)[row(cormat.r)[upper.tri(cormat.r)]], 
                      Var2 = colnames(cormat.r)[col(cormat.r)[upper.tri(cormat.r)]], 
                      corr=round(cormat.r[upper.tri(cormat.r)],4),
                      p = round(cormat.p[upper.tri(cormat.p)],4))

#filter relevant values
data.cor.rel <- data.cor[corr != 0 & corr != 1 & corr != -1 & p < 0.05]
```

** Graphical analysis of the recovery of all parameters**
For every parameters, dependencies from other variables will be checked. Depending on this, the plots are designed as meaningful as possible. 

```{r, echo=TRUE}
#recovery of b0
data.cor.rel[Var1 == "par_b0" | Var2 == "par_b0"]

data.w.b0 <- data.w %>% 
  group_by(type, true_par_tau) %>% 
  mutate(med_b0 = median(par_b0))

ggplot(data = data.w.b0,
       mapping = aes(y = par_b0)) +
  geom_histogram(bins = 100) +
  geom_hline(aes(yintercept = true_par_b0), color = "red") +
  geom_hline(aes(yintercept = med_b0), color="blue", linetype="dashed")+
  facet_grid(true_par_tau ~ type)+
  theme_minimal()


#recovery of b1
data.cor.rel[Var1 == "par_b1" | Var2 == "par_b1"]

data.w.b1 <- data.w %>% 
  group_by(type, true_par_tau) %>% 
  mutate(med_b1 = median(par_b1))

ggplot(data = data.w.b1,
       mapping = aes(y = par_b1)) +
  geom_histogram(bins = 100) +
  geom_hline(aes(yintercept = true_par_b1), color = "red") +
  geom_hline(aes(yintercept = med_b1), color="blue", linetype="dashed")+
  facet_grid(true_par_tau ~ type)
  theme_minimal()

#recovery of size
data.cor.rel[Var1 == "par_size" | Var2 == "par_size"]

data.w.size <- data.w %>% 
  group_by(type) %>% 
  mutate(med_size = median(par_size))

ggplot(data = data.w.size,
       mapping = aes(y = par_size)) +
  geom_histogram(bins = 100) +
  geom_hline(aes(yintercept = true_par_size), color = "red") +
  geom_hline(aes(yintercept = med_size, group = type), color="blue", linetype="dashed")+
  facet_wrap(~type)+
  theme_minimal()

#recovery of color
data.cor.rel[Var1 == "par_color" | Var2 == "par_color"]

data.w.color <- data.w %>% 
  group_by(nblock, true_par_lambda, true_par_tau) %>% 
  mutate(med_color = median(par_color))

ggplot(data = data.w.color,
       mapping = aes(y = par_color)) +
  geom_histogram(bins = 100) +
  geom_hline(aes(yintercept = true_par_color), color = "red") +
  geom_hline(aes(yintercept = med_color), color="blue", linetype="dashed")+
  facet_grid(true_par_lambda ~ nblock + true_par_tau)
  theme_minimal()

#recovery of shape
data.cor.rel[Var1 == "par_shape" | Var2 == "par_shape"]

data.w.shape <- data.w %>%
  group_by(true_par_tau, true_par_lambda, type) %>%
  mutate(med_shape = median(par_shape))

ggplot(data = data.w.shape,
       mapping = aes(y = par_shape)) +
  geom_histogram(bins = 100) +
  geom_hline(aes(yintercept = true_par_shape), color = "red") +
  geom_hline(aes(yintercept = med_shape), color="blue", linetype="dashed") +
  facet_grid(type ~ true_par_tau + true_par_lambda) +
  theme_minimal()

#recovery of lambda
data.cor.rel[Var1 == "par_lambda" | Var2 == "par_lambda"]

data.w.lambda <- data.w %>%
  group_by(true_par_lambda, true_par_tau, type) %>%
  mutate(med_lambda = median(par_lambda))
         
ggplot(data = data.w.lambda,
       mapping = aes(y = par_lambda)) +
  geom_histogram(bins = 100) +
  geom_hline(aes(yintercept = true_par_lambda), color = "red") +
  geom_hline(aes(yintercept = med_lambda), color="blue", linetype="dashed")+
  facet_grid(true_par_lambda + true_par_tau ~ type)+
  theme_minimal()

#recovery of tau
data.cor.rel[Var1 == "par_tau" | Var2 == "par_tau"]

data.w.tau <- data.w %>% 
  group_by(true_par_tau, nblock) %>% 
  mutate(med_tau = median(par_tau))
  
ggplot(data = data.w.tau,
       mapping = aes(y = par_tau)) +
  geom_histogram(bins = 100) +
  geom_hline(aes(yintercept = true_par_tau), color = "red") +
  geom_hline(aes(yintercept = med_tau), color="blue", linetype="dashed")+ 
  facet_grid(nblock ~ true_par_tau)
  theme_minimal()
```

```{r, echo=TRUE, results="hide", message=FALSE}
data.w.mlr <- data.w
#Z-transformation
data.w.mlr$Ztau <- scale(data.w.mlr$par_tau)
data.w.mlr$Ztrue_tau <- scale(data.w.mlr$true_par_tau)
```

## 2. Impact of omitting the first 3 and 8 rows on recovering tau

According to the GCM, omitting the first 3 or 8 rows should lead to better recovery of the tau parameter. Reason being, that the machine learning algorithm does not make accurate decisions on early rows of training.  To further test the effect of omitting the aforementioned rows, i compare their beta-weights. The intercepts can be ignored, since z-transformed variables are being used. A total of 3 comparisons will be made: 0 vs 3, 0 vs 8, 3 vs 8. 

**There is at least one significant variable that predicts Zdiscount**

```{r, echo=TRUE}
data.cor.rel[Var1 == "par_tau" | Var2 == "par_tau"]
```

**H1: The regression-beta coefficient of a regressionmodel with 0, resp. 3 rows omitted, signifficantly differs from a regressionmodel with 3 resp. 8 rows removed.**

```{r, echo=TRUE}
#data.frames for separate testing
data.w.mlr.30 <- data.w.mlr[nblock == 30]
data.w.mlr.100 <- data.w.mlr[nblock == 100]

#set up linear models
data.w.discount.lm.30 <- lm(par_tau ~ true_par_tau:factor(discount),
                        data = data.w.mlr.30)
data.w.discount.lm.100 <- lm(Ztau ~ Ztrue_tau:factor(discount),
                        data = data.w.mlr.100)

plot(data.w.discount.lm.30)
plot(data.w.discount.lm.100)

anova(data.w.discount.lm.30)
anova(data.w.discount.lm.100)


#slope coefficients
data.w.discount.coef.30 <- lstrends(data.w.discount.lm.30,
                                 "discount",
                                 var = "Ztrue_tau")
data.w.discount.coef.100 <- lstrends(data.w.discount.lm.100,
                                 "discount",
                                 var = "Ztrue_tau")

#compare slope coefficients
pairs(data.w.discount.coef.30)
pairs(data.w.discount.coef.100)

```

```{r, echo=TRUE}
#violin plots for discount per par_tau (Ev m端ssen wir noch konditionieren)
ggplot(data = data.w,
       mapping = aes(x = as.factor(discount),
                     y = par_tau)) +
  geom_violin(width = 0.9) +
  stat_summary(fun = "median", 
               geom = "point") +
  geom_line(aes(x = discount, 
                y = true_par_tau), 
            color = "red", 
            size = 0.5) +
  facet_grid(nblock ~ true_par_tau)+
  ylim(0, 10) +
  xlab("Discount") +
  ylab("Tau") +
  theme_minimal()

# linear regression
ggplot(data = data.w,
       mapping = aes(x = true_par_tau,
                     y = par_tau)) +
  geom_point(aes(x = true_par_tau,
                 y = par_tau),
             alpha = 1 / 20) +
  geom_line(aes(x = true_par_tau,
                y = true_par_tau),
            color = "red") +
  stat_summary(aes(group = discount),
               fun = median,
               geom = "line",
               color = c(rep("green", 6), #discount = 0
                         rep("blue", 6),  #discount = 8
                         rep("green", 6), #discount = 0
                         rep("blue", 6)   #discount = 8
                         ))+
  facet_wrap(~nblock) +
  ylim(0,10) +
  xlab("True tau") +
  ylab("Median of estimated tau") +
  theme_minimal()
```


## 3. Impact of training with multiple blocks on recovery of tau

According to the GCM, the more repetitive blocks a subject will be able to learn with, the better should the categorization accuracy get during. Therefore, with increasing blocks, a better recovery is expected. To evaluate the dependencies of the variable nblocks (and therefore the decision to test marginalized or conditionized), a multiple linear regression will be calculated in the first place. Then slope coefficients are being compared to see differences in recovery of parameter tau. Last, regressionlines and violin plots are drawn to illustrate the results.

**H1: One or more variables have an impact on Znblock**

```{r, echo=TRUE}
# decision between marginalized vs. conditionized testing
data.w.nblock.mlr <- lm(formula = nblock ~ Ztrue_tau + true_par_lambda + type + Zdiscount,
                        data = data.w.mlr)

summary(data.w.nblock.mlr)
```

**H1: The regression-beta coefficient of nblock = 30 resp. 50, signifficantly differs from a beta-weight from nblock = 50 resp. 100**

```{r, echo=TRUE}
#set up linear models
data.w.nblock.lm <- lm(Ztau ~ Ztrue_tau:Znblock,
                        data = data.w.mlr)

anova(data.w.discount.lm)

#slope coefficients
data.w.nblock.coef <- lstrends(data.w.nblock.lm,
                                 "Znblock",
                                 var = "Ztrue_tau")

#compare slope coefficients
pairs(data.w.nblock.coef)
```

```{r, echo=TRUE}
#violin plots for nblock per par_tau (Ev m端ssen wir noch konditionieren)
ggplot(data = data.w,
       mapping = aes(x = as.factor(true_par_nblock),
                     y = par_tau)) +
  geom_violin(width = 0.7) +
  stat_summary(fun = "median", 
               geom = "point") +
  geom_line(aes(x = true_par_nblock, 
                y = true_par_tau), 
            color = "red", 
            size = 0.5) +
  facet_wrap(~true_par_tau, 
             nrow = 2)+
  ylim(0, 10) +
  xlab("nblock") +
  ylab("Tau") +
  theme_minimal()

# linear regression
ggplot(data = results.w,
       mapping = aes(x = true_par_tau,
                     y = par_tau)) +
  geom_point(aes(x = true_par_tau,
                 y = par_tau),
             alpha = 1 / 5) +
  geom_line(aes(x = true_par_tau, y = true_par_tau), color = "red") +
  geom_line(data = results.w[true_par_nblock == 30],
            aes(x = true_par_tau,
                y = par_tau),
            color = "green") +
  geom_line(data = results.w[true_par_nblock == 50],
            aes(x = true_par_tau,
                y = par_tau),
            color = "blue") +
  geom_line(data = results.w[true_par_nblock == 100],
            aes(x = true_par_tau,
                y = par_tau),
            color = "yellow") +
  ylim(0,10) +
  xlab("True tau") +
  ylab("Median of estimated tau") +
  theme_minimal()
```

## 4. ability of the GCM to grasp the difficulty of the categories on the tau parameter

Shepard suggests, that of 70 possible combinations of elements characterized by 3 dimensions in psychological space, 6 basic types can be derived. The types vary in the dimensions, a subject has to focus on, and therefore can be put into an order of difficulty.Shepard suggests the following order of difficulty:  I < II < ( I I I , IV, V) < VI. If the GCM function works, depending on the type, goodness of recovery of tau paramter should imitate the suggested order. First I check for significant dependencies on the type variable, then i test for differences in beta weights of recovered tau, last i visually present the results.

**H1: One or more variables have an impact on Ztype**

```{r, echo=TRUE}
# decision between marginalized vs. conditionized testing
data.w.type.mlr <- lm(formula = Ztype ~ Ztrue_tau + Ztrue_lambda + Znblock + Zdiscount,
                        data = data.w.mlr)

summary(data.w.type.mlr)
```

**H1: The regression-beta coefficient of type, signifficantly differs from beta-weight of other types**

```{r, echo=TRUE}
#set up linear models
data.w.type.lm <- lm(Ztau ~ Ztrue_tau * Ztype,
                        data = data.w.mlr)

anova(data.w.discount.lm)

#slope coefficients
data.w.type.coef <- lstrends(data.w.type.lm,
                                 "Ztype",
                                 var = "Ztrue_tau")

#compare slope coefficients
pairs(data.w.type.coef)
```

```{r, echo=TRUE}
#violin plots for nblock per par_tau (Ev m端ssen wir noch konditionieren)
ggplot(data = data.w,
       mapping = aes(x = as.factor(true_par_type),
                     y = par_tau)) +
  geom_violin(width = 0.7) +
  stat_summary(fun = "median", 
               geom = "point") +
  geom_line(aes(x = true_par_type, 
                y = true_par_tau), 
            color = "red", 
            size = 0.5) +
  facet_wrap(~true_par_tau, 
             nrow = 2)+
  ylim(0, 10) +
  xlab("type") +
  ylab("Tau") +
  theme_minimal()

# linear regression
ggplot(data = data.w,
       mapping = aes(y = true_par_tau,
                     x = par_tau)) +
  geom_point(aes(y = true_par_tau,
                 x = par_tau),
             alpha = 1 / 5) +
  geom_line(aes(y = true_par_tau, x = true_par_tau), color = "red") +
  geom_line(aes(y = true_par_tau,
                x = par_tau,
                color = type)) +
  ylim(0,10) +
  ylab("True tau") +
  xlab("Estimated tau") +
  theme_minimal()

range(data.l)
```

## 5. Influence of Lambda on the recovery of Tau

Lambda, the speed of learning (sensitivity) is considered to have a positive impact on the recovery of Tau. When a person learns quicker, the recovery of the prabability of implementing an action should be better especially with bigger Tau values. First, i check, what variables have an impact on tau, then i compare beta-weights of lambda 1 and 5, last i visually summarize the results.

**H1: One or more variables have an impact on Zlambda**

```{r, echo=TRUE}
# decision between marginalized vs. conditionized testing
data.w.lambda.mlr <- lm(formula = Ztrue_lambda ~ Ztrue_tau + Ztype + Znblock + Zdiscount,
                        data = data.w.mlr)

summary(data.w.lambda.mlr)
```

**H1: The regression-beta coefficient of lambda = 1 signifficantly differs from beta of lambda = 5**

```{r, echo=TRUE}
#set up linear models
data.w.lambda.lm <- lm(Ztau ~ Ztrue_tau:Zlambda,
                        data = data.w.mlr)

anova(data.w.lambda.lm)

#slope coefficients
data.w.lambda.coef <- lstrends(data.w.type.lm,
                                 "Zlambda",
                                 var = "Ztrue_tau")

#compare slope coefficients
pairs(data.w.lambda.coef)
```

```{r, echo=TRUE}
#violin plots for nblock per par_tau (Ev m端ssen wir noch konditionieren)
ggplot(data = data.w,
       mapping = aes(x = as.factor(true_par_lambda),
                     y = par_tau)) +
  geom_violin(width = 0.7) +
  stat_summary(fun = "median", 
               geom = "point") +
  geom_line(aes(x = true_par_lambda, 
                y = true_par_tau), 
            color = "red", 
            size = 0.5) +
  facet_wrap(~true_par_tau, 
             nrow = 2)+
  ylim(0, 10) +
  xlab("type") +
  ylab("Tau") +
  theme_minimal()

# linear regression
ggplot(data = results.w,
       mapping = aes(x = true_par_tau,
                     y = par_tau)) +
  geom_point(aes(x = true_par_tau,
                 y = par_tau),
             alpha = 1 / 5) +
  geom_line(aes(x = true_par_tau, y = true_par_tau), color = "red") +
  geom_line(aes(x = true_par_tau,
                y = par_tau),
            color = aes(~true_par_lambda)) +
  ylim(0,10) +
  xlab("True tau") +
  ylab("Median of estimated tau") +
  theme_minimal()
```
